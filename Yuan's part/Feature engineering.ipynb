{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce File Size into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load the large CSV file\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the size of each chunk\n",
    "chunk_size = 500000  # This number can change \n",
    "\n",
    "# Split the CSV into chunks\n",
    "for i in range(0, len(data), chunk_size):\n",
    "    chunk = data.iloc[i:i + chunk_size]\n",
    "    chunk.to_csv(f'chunk_{i//chunk_size}.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chunk_0.csv\")\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要增加的部分\n",
    "\n",
    "1，如果一个人只浏览但没有点击，那么它的数据在排序中是否是无意义的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "df_cleaned_chunk0 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
      "0        1  2013-04-04 08:32:15       12                          187   \n",
      "1        1  2013-04-04 08:32:15       12                          187   \n",
      "2        1  2013-04-04 08:32:15       12                          187   \n",
      "3        1  2013-04-04 08:32:15       12                          187   \n",
      "4        1  2013-04-04 08:32:15       12                          187   \n",
      "\n",
      "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
      "0                      0.0                   0.0              219      893   \n",
      "1                      0.0                   0.0              219    10404   \n",
      "2                      0.0                   0.0              219    21315   \n",
      "3                      0.0                   0.0              219    27348   \n",
      "4                      0.0                   0.0              219    29604   \n",
      "\n",
      "   prop_starrating  prop_review_score  ...  comp7_rate  comp7_inv  \\\n",
      "0                3                3.5  ...         0.0        0.0   \n",
      "1                4                4.0  ...         0.0        0.0   \n",
      "2                3                4.5  ...         0.0        0.0   \n",
      "3                2                4.0  ...         0.0        0.0   \n",
      "4                4                3.5  ...         0.0        0.0   \n",
      "\n",
      "   comp7_rate_percent_diff  comp8_rate  comp8_inv  comp8_rate_percent_diff  \\\n",
      "0                      0.0         0.0        0.0                      0.0   \n",
      "1                      0.0         0.0        0.0                      0.0   \n",
      "2                      0.0         0.0        0.0                      0.0   \n",
      "3                      0.0        -1.0        0.0                      5.0   \n",
      "4                      0.0         0.0        0.0                      0.0   \n",
      "\n",
      "   click_bool  gross_bookings_usd  booking_bool  bool_visitor_hist  \n",
      "0           0                 0.0             0                  0  \n",
      "1           0                 0.0             0                  0  \n",
      "2           0                 0.0             0                  0  \n",
      "3           0                 0.0             0                  0  \n",
      "4           0                 0.0             0                  0  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned_chunk0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify according to different variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criteria_in_the_search_query = ['date_time', 'site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool']\n",
    "\n",
    "Static_hotel_characteristics = ['prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price','srch_query_affinity_score','orig_destination_distance']\n",
    "\n",
    "Dynamic_hotel_characteristics = ['random_bool', 'position', 'price_usd', 'promotion_flag','click_bool', 'gross_bookings_usd',\n",
    "       'booking_bool']\n",
    "\n",
    "Visitor_information = ['srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd']\n",
    "\n",
    "Competitive_information = ['comp1_rate', 'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "Other = []\n",
    "\n",
    "all_catogories = [Criteria_in_the_search_query, Static_hotel_characteristics, \n",
    "                 Dynamic_hotel_characteristics, Visitor_information,Competitive_information,Other]\n",
    "all_variables = []\n",
    "for i in all_catogories:\n",
    "    for j in i:\n",
    "        all_variables.append(j)\n",
    "\n",
    "df_variables = df.columns.to_list()\n",
    "\n",
    "differences = set(all_variables).symmetric_difference(set(df_variables))\n",
    "\n",
    "if differences == set():\n",
    "    print('All catogorized')\n",
    "else:\n",
    "    print(f'Differences between df_variables and catogorized variables: {differences}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria_in_the_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Criteria_in_the_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lambdarank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model(without parameter choose and feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 增加一个计算得分的列\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['score'] = df.apply(assign_scores, axis=1)\n",
    "\n",
    "features = ['site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool','prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price',\n",
    "\n",
    "       'srch_query_affinity_score','orig_destination_distance','random_bool', \n",
    "       'price_usd', 'promotion_flag','srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd','comp1_rate', \n",
    "\n",
    "       'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "# 划分数据集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_cleaned.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG Score: 0.3288798428994425\n"
     ]
    }
   ],
   "source": [
    "All_test_df = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv')\n",
    "\n",
    "# 预测测试集\n",
    "\n",
    "All_test_pred = bst.predict(All_test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "All_test_df['predictions'] = All_test_pred\n",
    "\n",
    "All_test_df['score'] = All_test_df.apply(assign_scores, axis=1)\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "All_test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = All_test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    # 确保每组数据中的预测是降序的（虽然已全局排序，这里是双重确认）\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "# 计算平均 NDCG 分数\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use all cleaned data without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With feature engineering by Menghan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_chunk_0.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 增加一个计算得分的列\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['score'] = df.apply(assign_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns\n",
    "\n",
    "# 使用列表推导式筛选出不需要删除的列\n",
    "features = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col\n",
    "]\n",
    "\n",
    "\n",
    "print(len(features))\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG Score: 0.7303758851941361\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_cleaned.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
