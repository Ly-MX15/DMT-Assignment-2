{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce File Size into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load the large CSV file\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the size of each chunk\n",
    "chunk_size = 500000  # This number can change \n",
    "\n",
    "# Split the CSV into chunks\n",
    "for i in range(0, len(data), chunk_size):\n",
    "    chunk = data.iloc[i:i + chunk_size]\n",
    "    chunk.to_csv(f'chunk_{i//chunk_size}.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chunk_0.csv\")\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要增加的部分\n",
    "\n",
    "1，如果一个人只浏览但没有点击，那么它的数据在排序中是否是无意义的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "df_cleaned_chunk0 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned_chunk0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify according to different variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criteria_in_the_search_query = ['date_time', 'site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool']\n",
    "\n",
    "Static_hotel_characteristics = ['prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price','srch_query_affinity_score','orig_destination_distance']\n",
    "\n",
    "Dynamic_hotel_characteristics = ['random_bool', 'position', 'price_usd', 'promotion_flag','click_bool', 'gross_bookings_usd',\n",
    "       'booking_bool']\n",
    "\n",
    "Visitor_information = ['srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd']\n",
    "\n",
    "Competitive_information = ['comp1_rate', 'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "Other = []\n",
    "\n",
    "all_catogories = [Criteria_in_the_search_query, Static_hotel_characteristics, \n",
    "                 Dynamic_hotel_characteristics, Visitor_information,Competitive_information,Other]\n",
    "all_variables = []\n",
    "for i in all_catogories:\n",
    "    for j in i:\n",
    "        all_variables.append(j)\n",
    "\n",
    "df_variables = df.columns.to_list()\n",
    "\n",
    "differences = set(all_variables).symmetric_difference(set(df_variables))\n",
    "\n",
    "if differences == set():\n",
    "    print('All catogorized')\n",
    "else:\n",
    "    print(f'Differences between df_variables and catogorized variables: {differences}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria_in_the_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Criteria_in_the_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lambdarank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model(without parameter choose and feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加一个计算得分的列\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Kaggle/train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['score'] = df.apply(assign_scores, axis=1)\n",
    "\n",
    "features = ['site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool','prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price',\n",
    "\n",
    "       'srch_query_affinity_score','orig_destination_distance','random_bool', \n",
    "       'price_usd', 'promotion_flag','srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd','comp1_rate', \n",
    "\n",
    "       'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "# 划分数据集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_cleaned.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_test_df = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv')\n",
    "\n",
    "# 预测测试集\n",
    "\n",
    "All_test_pred = bst.predict(All_test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "All_test_df['predictions'] = All_test_pred\n",
    "\n",
    "All_test_df['score'] = All_test_df.apply(assign_scores, axis=1)\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "All_test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = All_test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    # 确保每组数据中的预测是降序的（虽然已全局排序，这里是双重确认）\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "# 计算平均 NDCG 分数\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use all cleaned data without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With feature engineering by Menghan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_chunk_0.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 读取CSV文件的前100行\n",
    "df1 = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_chunk_0.csv', nrows=100)\n",
    "\n",
    "# 将这100行保存到一个新的CSV文件中\n",
    "df1.to_csv('feature_engineered_training_chunk_0_100.csv',index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加一个计算得分的列\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['score'] = df.apply(assign_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "\n",
    "# 使用列表推导式筛选出不需要删除的列\n",
    "features = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col\n",
    "]\n",
    "\n",
    "print(features)\n",
    "\n",
    "print(len(features))\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_cleaned.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_loss(y_true, y_pred):\n",
    "    # 假定y_true和y_pred的形状都是(batch_size, 1)\n",
    "    # 计算IDCG（理想情况下的NDCG分母）\n",
    "    sorted_true = tf.sort(y_true, direction='DESCENDING')\n",
    "    idcg = tf.math.reduce_sum((2 ** sorted_true - 1) / tf.math.log2(tf.range(2, tf.size(sorted_true) + 2, dtype=tf.float32)))\n",
    "\n",
    "    # 对预测值进行排序\n",
    "    _, indices = tf.nn.top_k(y_pred, k=tf.shape(y_pred)[0])\n",
    "    sorted_preds = tf.gather(y_true, indices)\n",
    "    \n",
    "    # 计算DCG\n",
    "    dcg = tf.math.reduce_sum((2 ** sorted_preds - 1) / tf.math.log2(tf.range(2, tf.size(sorted_preds) + 2, dtype=tf.float32)))\n",
    "    \n",
    "    # 计算NDCG\n",
    "    ndcg = dcg / idcg\n",
    "    # 损失是1-NDCG\n",
    "    return 1.0 - ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 文件路径和块大小\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_set_VU_DM.csv'\n",
    "chunk_size = 200000  # 根据可用内存调整大小\n",
    "\n",
    "# 读取数据并计算总块数\n",
    "total_chunks = sum(1 for _ in pd.read_csv(file_path, chunksize=chunk_size))\n",
    "train_chunk_count = int(total_chunks * 0.8)  # 用于训练的块的数量\n",
    "\n",
    "# 定义分数赋值函数\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 类别映射\n",
    "mapping = {0: 0, 1: 1, 5: 2}\n",
    "\n",
    "# 初始化标准化处理器\n",
    "scaler = StandardScaler()\n",
    "model_defined = False\n",
    "model = None\n",
    "\n",
    "# 读取数据块，进行训练和测试\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "    chunk['score'] = chunk.apply(assign_scores, axis=1).map(mapping)  # 应用评分函数并映射类别\n",
    "    chunk_features = [col for col in chunk.columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd', 'srch_id']]\n",
    "\n",
    "    # 标准化数据\n",
    "    if not model_defined:\n",
    "        scaler.fit(chunk[chunk_features])  # 只在第一个块上拟合scaler\n",
    "        model_defined = True\n",
    "    \n",
    "    chunk[chunk_features] = scaler.transform(chunk[chunk_features])\n",
    "\n",
    "    if i < train_chunk_count:  # 训练数据\n",
    "        if model is None:\n",
    "            # 定义模型\n",
    "            model = Sequential([\n",
    "                Dense(128, activation='relu', input_shape=(len(chunk_features),)),\n",
    "                Dropout(0.3),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='sigmoid')  # 假设为回归任务\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss=ndcg_loss)\n",
    "\n",
    "        # 训练模型\n",
    "        X_train = chunk[chunk_features]\n",
    "        y_train = chunk['score']\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "    else:  # 测试数据\n",
    "        X_test = chunk[chunk_features]\n",
    "        y_test = chunk['score']\n",
    "        predictions = model.predict(X_test)\n",
    "        chunk['predictions'] = predictions.flatten()\n",
    "\n",
    "        # 计算NDCG得分\n",
    "        chunk = chunk.sort_values(['srch_id', 'predictions'], ascending=[True, False])\n",
    "        grouped = chunk.groupby('srch_id')\n",
    "        ndcg_scores = [ndcg_score([group['score'].tolist()], [group['predictions'].tolist()]) for _, group in grouped if len(np.unique(group['score'])) > 1]\n",
    "        average_ndcg = np.mean(ndcg_scores)\n",
    "        print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final(using loss='sparse_categorical_crossentropy') Very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_set_VU_DM.csv'\n",
    "chunk_size = 200000  # 根据可用内存调整大小\n",
    "total_chunks = sum(1 for _ in pd.read_csv(file_path, chunksize=chunk_size))  # 计算总块数\n",
    "train_chunk_count = int(total_chunks * 0.8)  # 用于训练的块的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分数赋值函数\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 标准化处理器初始化\n",
    "scaler = StandardScaler()\n",
    "model_defined = False\n",
    "model = None\n",
    "\n",
    "# 读取数据块，训练和测试\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "    chunk['score'] = chunk.apply(assign_scores, axis=1) # 应用评分函数并映射类别\n",
    "    chunk_features = [col for col in chunk.columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd', 'srch_id']]\n",
    "    #if i < train_chunk_count:  # 训练数据\n",
    "    if i < 10:\n",
    "        if not model_defined:\n",
    "            model = Sequential([\n",
    "                Dense(128, activation='relu', input_shape=(len(chunk_features),)),\n",
    "                Dropout(0.3),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='linear')\n",
    "            ])\n",
    "            model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',  # 使用 MSE 作为损失函数\n",
    "              metrics=['mean_squared_error'])  # 评估指标使用\n",
    "            model_defined = True\n",
    "        # 标准化训练数据\n",
    "        scaler.partial_fit(chunk[chunk_features])  # 计算标准化参数\n",
    "        X_train = scaler.transform(chunk[chunk_features])\n",
    "        y_train = chunk['score'].values\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "        model.fit(train_dataset, epochs=1)\n",
    "    if i == 11 or i == 12:    \n",
    "    #else:  # 测试数据\n",
    "        X_test = scaler.transform(chunk[chunk_features])\n",
    "        y_test = chunk['score'].values\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "        predictions = model.predict(test_dataset)\n",
    "        chunk['predictions'] = np.argmax(predictions, axis=1)  # 获取概率最高的类别\n",
    "\n",
    "        # 按 srch_id 和 predictions 降序排序\n",
    "        chunk.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        # 计算 NDCG 分数\n",
    "        grouped = chunk.groupby('srch_id')\n",
    "        ndcg_scores = []\n",
    "        for name, group in grouped:\n",
    "            true_relevance = group['score'].values\n",
    "            scores_pred = group['predictions'].values\n",
    "            if len(np.unique(true_relevance)) > 1:\n",
    "                ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "        average_ndcg = np.mean(ndcg_scores)\n",
    "        print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_set_VU_DM.csv'\n",
    "chunk_size = 100000  # 根据可用内存调整大小\n",
    "\n",
    "\n",
    "# 首次读取数据，只为获取全部的 srch_id 以进行分层抽样\n",
    "srch_ids = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size, usecols=['srch_id']):\n",
    "    srch_ids = pd.concat([srch_ids, chunk['srch_id']])\n",
    "srch_id_counts = srch_ids.value_counts()\n",
    "train_ids, test_ids = train_test_split(srch_id_counts.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化数据框以存储训练和测试数据\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "# 读取数据并根据划分的srch_id分到训练集或测试集中\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    chunk['score'] = chunk.apply(assign_scores, axis=1)  # 应用评分函数\n",
    "    train_chunk = chunk[chunk['srch_id'].isin(train_ids)]\n",
    "    test_chunk = chunk[chunk['srch_id'].isin(test_ids)]\n",
    "    train_df = pd.concat([train_df, train_chunk])\n",
    "    test_df = pd.concat([test_df, test_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "features = [col for col in train_df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score'] and 'gross_bookings_usd' not in col]\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: 0, 1: 1, 5: 2}\n",
    "train_df['score'] = train_df['score'].map(mapping)\n",
    "test_df['score'] = test_df['score'].map(mapping)\n",
    "\n",
    "# 模型构建\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 更改为3，因为现在有三个类别\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[features].values, train_df['score'].values)).batch(32)\n",
    "model.fit(train_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df[features].values, test_df['score'].values)).batch(32)\n",
    "test_df['predictions'] = model.predict(test_dataset).flatten()\n",
    "\n",
    "# 按 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 计算 NDCG 分数\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "for name, group in grouped:\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    if len(np.unique(true_relevance)) > 1:\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_chunk_0.csv'\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/feature_engineered_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.apply(assign_scores, axis=1)\n",
    "columns = df.columns\n",
    "\n",
    "# 使用列表推导式筛选出不需要删除的列\n",
    "features = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 特征标准化\n",
    "scaler = StandardScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])\n",
    "\n",
    "# 创建 TensorFlow 数据集\n",
    "def make_dataset(data_df):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data_df[features].values, data_df['score'].values))\n",
    "    dataset = dataset.shuffle(buffer_size=len(data_df)).batch(32).repeat()\n",
    "    return dataset\n",
    "\n",
    "train_dataset = make_dataset(train_df)\n",
    "test_dataset = make_dataset(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  # 输出层，无激活函数\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 模型训练\n",
    "model.fit(train_dataset, epochs=10, steps_per_epoch=200, validation_data=test_dataset, validation_steps=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集\n",
    "test_df['predictions'] = model.predict(test_df[features])\n",
    "\n",
    "# 按 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model NN using TensorFlow Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = 'D:/Table/P5/DM-AS2/Test_data/training/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(2):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征\n",
    "unique_ids = df['srch_id'].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_data = df[df['srch_id'].isin(train_ids)]\n",
    "test_data = df[df['srch_id'].isin(test_ids)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[feature_columns] = scaler.fit_transform(train_data[feature_columns])\n",
    "test_data[feature_columns] = scaler.fit_transform(test_data[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = 'D:/Table/P5/DM-AS2/Test_data/training/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(2):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        data = df_chunk\n",
    "    if i == 1:\n",
    "        test = df_chunk\n",
    "    else:\n",
    "        data = pd.concat([data, df_chunk], axis=0)\n",
    "\n",
    "# 2. 数据预处理\n",
    "# 定义得分\n",
    "feature_columns = [col for col in data.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "'''\n",
    "unique_ids = data['srch_id'].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)\n",
    "#train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "train_data = data[data['srch_id'].isin(train_ids)]\n",
    "test_data = data[data['srch_id'].isin(test_ids)]\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
    "\n",
    "train_data = data\n",
    "test_data = test\n",
    "\n",
    "# 3. 构建按 srch_id 分组的测试数据\n",
    "grouped_test_data = test_data.groupby('srch_id').apply(\n",
    "    lambda x: x.sort_values(by='score', ascending=False)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 4. 创建 TensorFlow Ranking 模型输入函数\n",
    "def input_fn(data, batch_size=32):\n",
    "    def _fn():\n",
    "        # Prepare features as a dictionary mapping from feature names to tensors.\n",
    "        # Ensure these names and structures match what the model expects.\n",
    "        features = {k: tf.reshape(tf.convert_to_tensor(data[k].values), (-1, 1)) for k in feature_columns}\n",
    "        labels = tf.cast(tf.convert_to_tensor(data['score'].values), tf.float32)\n",
    "        #print(\"Labels dtype:\", labels.dtype)\n",
    "        # Batch the data\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        #print(dataset)\n",
    "        return dataset\n",
    "    return _fn\n",
    "\n",
    "# 5. 定义模型\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.1)\n",
    "loss = tfr.keras.losses.get(tfr.losses.RankingLossKey.APPROX_NDCG_LOSS)\n",
    "model = tfr.keras.model.create_keras_model(\n",
    "    network=tfr.keras.canned.DNNRankingNetwork(\n",
    "        context_feature_columns=None,\n",
    "        example_feature_columns={col: tf.feature_column.numeric_column(col) for col in feature_columns},\n",
    "        hidden_layer_dims=[128, 64],\n",
    "        activation=tf.nn.relu,\n",
    "        dropout=0.5),\n",
    "    loss=loss,\n",
    "    metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")],\n",
    "    optimizer=optimizer,\n",
    "    size_feature_name=None)\n",
    "\n",
    "# 6. 训练模型\n",
    "train_input_fn = input_fn(train_data)\n",
    "model.fit(train_input_fn(), steps_per_epoch=1000, epochs = 10)\n",
    "\n",
    "# 7. 评估模型 - 计算按 srch_id 分组的 NDCG\n",
    "test_input_fn = input_fn(grouped_test_data)\n",
    "predictions = model.predict(test_input_fn())\n",
    "\n",
    "# 将预测与真实得分按 srch_id 添加到数据帧，然后计算 NDCG\n",
    "grouped_test_data['predictions'] = predictions.flatten()\n",
    "grouped = grouped_test_data.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "for name, group in grouped:\n",
    "    true_labels = group['score'].tolist()\n",
    "    pred_labels = group['predictions'].tolist()\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_labels)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_labels], [pred_labels], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score across groups: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "train_data[feature_columns] = scaler.fit_transform(train_data[feature_columns])\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.1)\n",
    "loss = tfr.keras.losses.get(tfr.losses.RankingLossKey.APPROX_NDCG_LOSS)\n",
    "model = tfr.keras.model.create_keras_model(\n",
    "    network=tfr.keras.canned.DNNRankingNetwork(\n",
    "        context_feature_columns=None,\n",
    "        example_feature_columns={col: tf.feature_column.numeric_column(col) for col in feature_columns},\n",
    "        hidden_layer_dims=[128, 64],\n",
    "        activation=tf.nn.relu,\n",
    "        dropout=0.1),\n",
    "    loss=loss,\n",
    "    metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")],\n",
    "    optimizer=optimizer,\n",
    "    size_feature_name=None)\n",
    "# 6. 训练模型\n",
    "train_input_fn = input_fn(train_data)\n",
    "model.fit(train_input_fn(), steps_per_epoch=1000, epochs=10)\n",
    "print(1)\n",
    "# 7. 评估模型 - 计算按 srch_id 分组的 NDCG\n",
    "test_input_fn = input_fn(grouped_test_data)\n",
    "predictions = model.predict(test_input_fn())\n",
    "\n",
    "# 将预测与真实得分按 srch_id 添加到数据帧，然后计算 NDCG\n",
    "grouped_test_data['predictions'] = predictions.flatten()\n",
    "grouped = grouped_test_data.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "for name, group in grouped:\n",
    "    true_labels = group['score'].tolist()\n",
    "    pred_labels = group['predictions'].tolist()\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_labels)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_labels], [pred_labels], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score across groups: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return int(5)\n",
    "    elif row['click_bool'] == 1:\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 数据文件路径\n",
    "base_path = 'D:/Table/P5/DM-AS2/Test_data/training/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "\n",
    "df = pd.read_csv(base_path + file_pattern.format(0), nrows = 50000)\n",
    "    \n",
    "\n",
    "print(0)\n",
    "# 数据预处理，定义分数\n",
    "df['score'] = df.apply(lambda row: 5 if row['booking_bool'] == 1 else 1 if row['click_bool'] == 1 else 0, axis=1)\n",
    "exclude_columns = ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']\n",
    "features = [col for col in df.columns if col not in exclude_columns]\n",
    "print(1)\n",
    "# 分割数据集\n",
    "X_train, X_test, y_train, y_test, srch_id_train, srch_id_test = train_test_split(df[features], df['score'], df['srch_id'], test_size=0.20, random_state=42)\n",
    "print(2)\n",
    "# 标准化数据\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 初始化模型\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "#model.fit(X_train_scaled, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "print(3)\n",
    "# 进行预测\n",
    "#predictions = model.predict(X_test_scaled)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 计算均方误差\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# 计算 NDCG\n",
    "# 根据 srch_id 分组并计算每个组的 NDCG\n",
    "df_test = pd.DataFrame({'srch_id': srch_id_test, 'y_test': y_test, 'predictions': predictions})\n",
    "ndcg_scores = []\n",
    "\n",
    "# 分组并为每个搜索 ID 排序预测结果\n",
    "for name, group in df_test.groupby('srch_id'):\n",
    "    true_relevance = group.sort_values('predictions', ascending=False)['y_test'].tolist()\n",
    "    scores_pred = group.sort_values('predictions', ascending=False)['predictions'].tolist()\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "# 计算平均 NDCG 分数\n",
    "average_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'D:/Table/P5/DM-AS2/Test_data/training/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "\n",
    "df = pd.read_csv(base_path + file_pattern.format(8), nrows = 200000)\n",
    "df['score'] = df.apply(lambda row: 5 if row['booking_bool'] == 1 else 1 if row['click_bool'] == 1 else 0, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, srch_id_train, srch_id_test = train_test_split(df[features], df['score'], df['srch_id'], test_size=0.20, random_state=42)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 计算均方误差\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# 计算 NDCG\n",
    "# 根据 srch_id 分组并计算每个组的 NDCG\n",
    "df_test = pd.DataFrame({'srch_id': srch_id_test, 'y_test': y_test, 'predictions': predictions})\n",
    "ndcg_scores = []\n",
    "\n",
    "# 分组并为每个搜索 ID 排序预测结果\n",
    "for name, group in df_test.groupby('srch_id'):\n",
    "    true_relevance = group.sort_values('predictions', ascending=False)['y_test'].tolist()\n",
    "    scores_pred = group.sort_values('predictions', ascending=False)['predictions'].tolist()\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "# 计算平均 NDCG 分数\n",
    "average_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('D:/Table/P5/DM-AS2/Test_data/best_feature_engineered_test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk[features] = chunk[features].fillna(0)\n",
    "    chunk_pred = model.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)\n",
    "\n",
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=RF.csv', index=False, header=True)\n",
    "\n",
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
