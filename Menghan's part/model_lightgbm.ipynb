{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here it's workable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = '/Users/eva/Documents/Study/Y1S2/DMT/assignment2/'\n",
    "file_pattern = 'feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(10):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "\n",
    "# 使用列表推导式筛选出不需要删除的列\n",
    "features = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col\n",
    "]\n",
    "\n",
    "\n",
    "print(len(features))\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查df NA 展示有NA的列\n",
    "na_counts = df.isna().sum()\n",
    "\n",
    "# 打印有NA值的列和NA值的数量\n",
    "print(\"Columns with NA values and their counts:\")\n",
    "for col, count in na_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count}\")\n",
    "# 删除有NA值的列\n",
    "df = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 40,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'min_child_samples': 21,\n",
    "    'min_child_weight': 0.001,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 2,\n",
    "    'reg_alpha': 0.001,\n",
    "    'reg_lambda': 8,\n",
    "    'cat_smooth': 0,\n",
    "    'num_iterations': 200,\n",
    "    'is_unbalance': True  # 仅当你确信数据不平衡严重且影响模型性能时才设置\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('/Users/eva/Documents/Study/Y1S2/DMT/assignment2/feature_engineered_test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_featured_cleaned(train+set_all_grouped_by).csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
