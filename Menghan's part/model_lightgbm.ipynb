{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce File Size into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load the large CSV file\n",
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the size of each chunk\n",
    "chunk_size = 500000  # This number can change \n",
    "\n",
    "# Split the CSV into chunks\n",
    "for i in range(0, len(data), chunk_size):\n",
    "    chunk = data.iloc[i:i + chunk_size]\n",
    "    chunk.to_csv(f'chunk_{i//chunk_size}.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chunk_0.csv\")\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要增加的部分\n",
    "\n",
    "1，如果一个人只浏览但没有点击，那么它的数据在排序中是否是无意义的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "df_cleaned_chunk0 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned_chunk0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify according to different variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criteria_in_the_search_query = ['date_time', 'site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool']\n",
    "\n",
    "Static_hotel_characteristics = ['prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price','srch_query_affinity_score','orig_destination_distance']\n",
    "\n",
    "Dynamic_hotel_characteristics = ['random_bool', 'position', 'price_usd', 'promotion_flag','click_bool', 'gross_bookings_usd',\n",
    "       'booking_bool']\n",
    "\n",
    "Visitor_information = ['srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd']\n",
    "\n",
    "Competitive_information = ['comp1_rate', 'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "Other = []\n",
    "\n",
    "all_catogories = [Criteria_in_the_search_query, Static_hotel_characteristics, \n",
    "                 Dynamic_hotel_characteristics, Visitor_information,Competitive_information,Other]\n",
    "all_variables = []\n",
    "for i in all_catogories:\n",
    "    for j in i:\n",
    "        all_variables.append(j)\n",
    "\n",
    "df_variables = df.columns.to_list()\n",
    "\n",
    "differences = set(all_variables).symmetric_difference(set(df_variables))\n",
    "\n",
    "if differences == set():\n",
    "    print('All catogorized')\n",
    "else:\n",
    "    print(f'Differences between df_variables and catogorized variables: {differences}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria_in_the_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Criteria_in_the_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lambdarank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model(without parameter choose and feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/chunk_0_cleaned.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 增加一个计算得分的列\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['score'] = df.apply(assign_scores, axis=1)\n",
    "\n",
    "features = ['site_id','srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
    "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
    "       'srch_saturday_night_bool','prop_country_id',\n",
    "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "       'prop_location_score1', 'prop_location_score2',\n",
    "       'prop_log_historical_price',\n",
    "\n",
    "       'srch_query_affinity_score','orig_destination_distance','random_bool', \n",
    "       'price_usd', 'promotion_flag','srch_id', 'visitor_location_country_id',\n",
    "       'visitor_hist_starrating', 'visitor_hist_adr_usd','comp1_rate', \n",
    "\n",
    "       'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff']\n",
    "\n",
    "# 划分数据集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_cleaned.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_test_df = pd.read_csv('C:/Users/98398/Desktop/P5/DM-AS2/Data/training_set_VU_DM.csv')\n",
    "\n",
    "# 预测测试集\n",
    "\n",
    "All_test_pred = bst.predict(All_test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "All_test_df['predictions'] = All_test_pred\n",
    "\n",
    "All_test_df['score'] = All_test_df.apply(assign_scores, axis=1)\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "All_test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = All_test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    # 确保每组数据中的预测是降序的（虽然已全局排序，这里是双重确认）\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "# 计算平均 NDCG 分数\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use all cleaned data without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With feature engineering by Menghan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = '/Users/eva/Documents/Study/Y1S2/DMT/assignment2/'\n",
    "file_pattern = 'feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(10):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#file_path = '/Users/eva/Documents/Study/Y1S2/DMT/assignment2/feature_engineered_training_set_VU_DM.csv'\n",
    "\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/cleaned_training_set_VU_DM.csv'\n",
    "#file_path = 'C:/Users/98398/Desktop/P5/DM-AS2/Test_data/training_set_VU_DM.csv'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# 增加一个计算得分的列\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "\n",
    "# 使用列表推导式筛选出不需要删除的列\n",
    "features = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col\n",
    "]\n",
    "\n",
    "\n",
    "print(len(features))\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查df NA 展示有NA的列\n",
    "na_counts = df.isna().sum()\n",
    "\n",
    "# 打印有NA值的列和NA值的数量\n",
    "print(\"Columns with NA values and their counts:\")\n",
    "for col, count in na_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count}\")\n",
    "# 删除有NA值的列\n",
    "df = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 40,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'min_child_samples': 21,\n",
    "    'min_child_weight': 0.001,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 2,\n",
    "    'reg_alpha': 0.001,\n",
    "    'reg_lambda': 8,\n",
    "    'cat_smooth': 0,\n",
    "    'num_iterations': 200,\n",
    "    'is_unbalance': True  # 仅当你确信数据不平衡严重且影响模型性能时才设置\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune max_depth and num_leaves on whole df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial   \n",
    "    \n",
    "\n",
    "def function_to_get_data(df):\n",
    "    sample_df = df.sample(frac=0.3, random_state=42)\n",
    "    train_df, test_df = train_test_split(sample_df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "    \n",
    "    x_train = train_df[features]\n",
    "    x_val = test_df[features]\n",
    "    y_train = train_df['score']\n",
    "    y_val = test_df['score']\n",
    "    \n",
    "    # we need group because we need to rank a list of item not like classification or regression\n",
    "    group_qids_train = train_df['srch_id'].value_counts().sort_index()\n",
    "    group_qids_val = test_df['srch_id'].value_counts().sort_index()\n",
    "    return x_train, x_val, y_train, y_val, group_qids_train, group_qids_val\n",
    "    \n",
    "def get_experience(x_train, x_val, y_train, y_val, group_qids_train, group_qids_val, ranking_param_grid):\n",
    "    params = {\n",
    "        'learning_rate': ranking_param_grid[0],\n",
    "        'n_estimators': ranking_param_grid[1],\n",
    "        'num_leaves': ranking_param_grid[2],\n",
    "        'max_depth': ranking_param_grid[3],\n",
    "        'boosting_type': ranking_param_grid[4]\n",
    "    }\n",
    "    gbm = lgb.LGBMRanker(n_jobs=24, **params)\n",
    "    \n",
    "    # eval_set = [10, 50, 100] this is the define NDCG@10, NDCG@50, NDCG@100\n",
    "    gbm.fit(x_train, y_train, group=group_qids_train, eval_set=[(x_val, y_val)],\n",
    "            eval_metric='ndcg', eval_group=[group_qids_val], eval_at=[10, 50, 100],\n",
    "            early_stopping_rounds=100, verbose=False)\n",
    "        \n",
    "    return gbm\n",
    "\n",
    "def fit_lgbm(df):\n",
    "    \"\"\"Train Light GBM model\"\"\"\n",
    "    ranking_param_grid = {\n",
    "        'learning_rate': np.arange(0.06, 0.14, 0.02),\n",
    "        'n_estimators': np.arange(100, 500, 100),\n",
    "        'num_leaves': range(25, 35, 1),\n",
    "        'max_depth': [-1, 50, 100, 200],\n",
    "        'boosting_type': ['gbdt', 'goss']\n",
    "    }\n",
    "\n",
    "    x_train, x_val, y_train, y_val, group_qids_train, group_qids_val = function_to_get_data(df)\n",
    "\n",
    "    the_best_configs = {}\n",
    "    len_ = 1\n",
    "\n",
    "    for k in ranking_param_grid.keys():\n",
    "        len_ *= len(ranking_param_grid[k])\n",
    "        \n",
    "    list_params = itertools.product(ranking_param_grid['learning_rate'],\n",
    "                                    ranking_param_grid['n_estimators'],\n",
    "                                    ranking_param_grid['num_leaves'],\n",
    "                                    ranking_param_grid['max_depth'],\n",
    "                                    ranking_param_grid['boosting_type'])\n",
    "    \n",
    "    # Multi processing\n",
    "    process_pool = mp.Pool(processes=32)\n",
    "    get_experience_each = partial(\n",
    "        get_experience,\n",
    "        x_train,\n",
    "        x_val,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        group_qids_train,\n",
    "        group_qids_val\n",
    "    )\n",
    "    with tqdm(total=len_) as pbar:\n",
    "        for each_item in process_pool.imap(\n",
    "            get_experience_each,\n",
    "            list_params\n",
    "        ):\n",
    "            pbar.update()\n",
    "            the_best_configs[list(each_item.best_score_['valid_0'].values())[\n",
    "                2]] = each_item.get_params()\n",
    "\n",
    "    df = pd.DataFrame(the_best_configs.items(), columns=['scores', 'params'])\n",
    "    best_params = df[df.scores == max(df.scores)].params.to_list()\n",
    "    print(best_params)\n",
    "        \n",
    "    return best_params\n",
    "\n",
    "best_params = fit_lgbm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, ndcg_score\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "sample_df = train_df.sample(frac=0.3, random_state=42)\n",
    "features = [col for col in sample_df.columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'srch_id'] and 'gross_bookings_usd' not in col and 'position' not in col]\n",
    "train_data = lgb.Dataset(sample_df[features], label=sample_df['score'], group=sample_df['srch_id'])\n",
    "\n",
    "\n",
    "# 使用 GroupKFold 以确保相同的查询 ID 不会同时出现在训练和验证集中\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# 设置 GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb.LGBMRanker(**default_params),\n",
    "    param_grid={\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'num_leaves': [31, 41, 51]\n",
    "    },\n",
    "    scoring=make_scorer(ndcg_score, needs_proba=True, k=5),\n",
    "    cv=gkf.split(X=train_data.data, y=train_data.label, groups=train_data.group),\n",
    "    verbose=3,\n",
    "    n_jobs=-1# 在发生错误时引发异常\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 执行网格搜索\n",
    "grid_search.fit(train_data.data, train_data.label, groups=train_data.group)\n",
    "\n",
    "# 输出最佳参数和分数\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best NDCG score: \", grid_search.best_score_)\n",
    "\n",
    "# 使用最佳参数重新训练模型\n",
    "best_params = grid_search.best_params_\n",
    "best_params.update(default_params)  # 确保这里是之前定义的 default_params\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# 准备数据\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['srch_id'])\n",
    "sample_df = train_df.sample(frac=0.2, random_state=42)\n",
    "features = [col for col in sample_df.columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'srch_id'] and 'gross_bookings_usd' not in col and 'position' not in col]\n",
    "X = sample_df[features].values\n",
    "y = sample_df['score'].values\n",
    "groups = sample_df['srch_id']\n",
    "\n",
    "# LightGBM 参数网格\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],  # 可以展开更多的选项，如[0.05, 0.1, 0.2]\n",
    "    'n_estimators': [100],  # 同样，可以根据需要增加更多的选择\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'num_leaves': [5, 6, 7, 12, 13, 14, 15, 28, 29, 30, 31],\n",
    "    'min_child_samples': [30, 40, 50],\n",
    "    'min_child_weight': [0.01],  # 这里只有一个值，如果需要测试不同的值可以增加\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0.01, 0.1, 1, 10, 100],  # loguniform通常用于生成一个范围的值，这里需要手动指定一系列的值\n",
    "    'reg_lambda': [0.01, 0.1, 1, 10, 100]  # 同上，指定一系列可能的值\n",
    "}\n",
    "\n",
    "def ndcg_scorer(y_true, y_pred):\n",
    "    return np.mean(lgb.Dataset.get_field('ndcg@5'))\n",
    "\n",
    "# 设置模型\n",
    "model = lgb.LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg',\n",
    "    verbose=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 创建 GroupKFold 以确保相同的查询 ID 不会同时出现在训练和验证集中\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# 设置 RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # 进行100次随机采样\n",
    "    scoring=make_scorer(ndcg_scorer, needs_proba=True),\n",
    "    cv=gkf,\n",
    "    verbose=3,\n",
    "    n_jobs=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行随机搜索\n",
    "random_search.fit(X, y, groups=groups)\n",
    "\n",
    "# 输出最佳参数和分数\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best NDCG score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳参数重新训练模型\n",
    "best_params = random_search.best_params_\n",
    "best_params.update({'objective': 'lambdarank', 'metric': 'ndcg'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBBO:\n",
    "    def __init__(self, fp_path, **kwargs):\n",
    "        self.fp_path = fp_path\n",
    "        self.iter = 0\n",
    "        self.train_set = None\n",
    "\n",
    "        self.kfold = kwargs.get('kfold', 3)\n",
    "        self.n_estimators = kwargs.get('n_estimators', 800)\n",
    "\n",
    "        csv_conn = open(self.fp_path, 'w')\n",
    "        writer = csv.writer(csv_conn)\n",
    "        writer.writerow(['loss', 'train_auc', 'valid_auc', 'train_ks', 'valid_ks',\n",
    "                         'lst_train_auc', 'lst_valid_auc', 'lst_train_ks', 'lst_valid_ks',\n",
    "                         'params', 'iteration', 'train_time'])\n",
    "        csv_conn.close()\n",
    "\n",
    "    def load_data(self, df_data, feature_list, label):\n",
    "        self.df_data = df_data.reset_index(drop=True)\n",
    "        self.feature_list = feature_list\n",
    "        self.label = label\n",
    "\n",
    "    def objective(self, params):\n",
    "        def eval_ks(ytrue, yprob):\n",
    "            fpr, tpr, thr = roc_curve(ytrue, yprob)\n",
    "            ks = max(tpr - fpr)\n",
    "            return \"ks\", ks, True\n",
    "\n",
    "        def eval_auc(ytrue, yprob):\n",
    "            auc = roc_auc_score(ytrue, yprob)\n",
    "            return \"auc\", auc, True\n",
    "\n",
    "        self.iter += 1\n",
    "        start = timer()\n",
    "        model = lgb.LGBMClassifier(**params,\n",
    "                                   learning_rate=0.1,\n",
    "                                   min_child_samples=20000,\n",
    "                                   objective='cross_entropy',\n",
    "                                   importance_type='gain',\n",
    "                                   class_weight='balanced',\n",
    "                                   boosting_type='gbdt', n_estimators=self.n_estimators,\n",
    "                                   silent=True, n_jobs=1, random_state=0\n",
    "                                   )\n",
    "\n",
    "        lst_train_auc, lst_valid_auc = list(), list()\n",
    "        lst_train_ks, lst_valid_ks = list(), list()\n",
    "\n",
    "        for k in range(self.kfold):\n",
    "            df_cv_train = self.df_data[self.df_data[f\"cv{k}\"] == 'train']\n",
    "            df_cv_valid = self.df_data[self.df_data[f\"cv{k}\"] == 'valid']\n",
    "            print(k, df_cv_train.shape, df_cv_valid.shape)\n",
    "            print(params)\n",
    "\n",
    "            eval_set = [(df_cv_train[self.feature_list], df_cv_train[self.label]),\n",
    "                        (df_cv_valid[self.feature_list], df_cv_valid[self.label])\n",
    "                        ]\n",
    "\n",
    "            model.fit(df_cv_train[self.feature_list], \n",
    "                    df_cv_train[self.label],\n",
    "                    eval_set=eval_set,  # 确保这里至少有一个元组组成的列表\n",
    "                    eval_metric='auc',  # 这里示例使用 'auc' 作为评估指标\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose=20)\n",
    "\n",
    "            yprob = model.predict_proba(df_cv_train[self.feature_list])[:, 1]\n",
    "            _, train_auc, _ = eval_auc(df_cv_train[self.label], yprob)\n",
    "            _, train_ks, _ = eval_ks(df_cv_train[self.label], yprob)\n",
    "            yprob = model.predict_proba(df_cv_valid[self.feature_list])[:, 1]\n",
    "            _, valid_auc, _ = eval_auc(df_cv_valid[self.label], yprob)\n",
    "            _, valid_ks, _ = eval_ks(df_cv_valid[self.label], yprob)\n",
    "\n",
    "            lst_train_auc.append(train_auc)\n",
    "            lst_valid_auc.append(valid_auc)\n",
    "            lst_train_ks.append(train_ks)\n",
    "            lst_valid_ks.append(valid_ks)\n",
    "\n",
    "            print(train_auc, valid_auc, train_ks, valid_ks)\n",
    "\n",
    "        run_time = timer() - start\n",
    "\n",
    "        train_auc_avg = np.mean(lst_train_auc)\n",
    "        valid_auc_avg = np.mean(lst_valid_auc)\n",
    "        train_ks_avg = np.mean(lst_train_ks)\n",
    "        valid_ks_avg = np.mean(lst_valid_ks)\n",
    "\n",
    "        loss = -valid_ks_avg\n",
    "\n",
    "        csv_conn = open(self.fp_path, 'a')\n",
    "        writer = csv.writer(csv_conn)\n",
    "\n",
    "        writer.writerow([loss,\n",
    "                         train_auc_avg, valid_auc_avg,\n",
    "                         train_ks_avg, valid_ks_avg,\n",
    "                         lst_train_auc, lst_valid_auc,\n",
    "                         lst_train_ks, lst_valid_ks,\n",
    "                         params, self.iter, run_time])\n",
    "\n",
    "        res = {'loss': loss,\n",
    "               'train_auc': train_auc_avg, 'valid_auc': valid_auc_avg,\n",
    "               'train_ks': train_ks_avg, 'valid_ks': valid_ks_avg,\n",
    "               'lst_train_auc': lst_train_auc, 'lst_valid_auc': lst_valid_auc,\n",
    "               'lst_train_ks': lst_train_ks, 'lst_valid_ks': lst_valid_ks,\n",
    "               'params': params, 'iteration': self.iter, 'train_time': run_time,\n",
    "               'status': STATUS_OK}\n",
    "\n",
    "        print(self.iter)\n",
    "        print(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def optimize(self, max_evals):\n",
    "        self.iter = 0\n",
    "\n",
    "        space = {\n",
    "            'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "            'num_leaves': hp.choice('num_leaves', [5, 6, 7, 12, 13, 14, 15, 28, 29, 30, 31]),\n",
    "            'subsample': hp.choice('subsample', [0.8, 0.9, 1.0]),\n",
    "            'colsample_bytree': hp.choice('colsample_bytree', [0.8, 0.9, 1.0]),\n",
    "            'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(1000)),\n",
    "            'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(1000))\n",
    "        }\n",
    "\n",
    "        rstate = np.random.default_rng(0)\n",
    "        best = fmin(fn=self.objective, space=space, algo=tpe.suggest, max_evals=max_evals,\n",
    "                    trials=Trials(), rstate=rstate)\n",
    "\n",
    "        print(best)\n",
    "        return best\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "for k, (itrain, ivalid) in enumerate(kf.split(sample_df[features], sample_df['score'])):\n",
    "    sample_df[f\"cv{k}\"] = None\n",
    "    sample_df.loc[itrain, f'cv{k}'] = 'train'\n",
    "    sample_df.loc[ivalid, f'cv{k}'] = 'valid'\n",
    "\n",
    "print('start tuning...')\n",
    "bo = LGBBO(f'param.csv')\n",
    "bo.load_data(sample_df, features, 'score')\n",
    "bo.optimize(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here it's workable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 准备 LightGBM 数据结构\n",
    "train_data = lgb.Dataset(train_df[features], label=train_df['score'], group=train_df['srch_id'].value_counts().sort_index())\n",
    "test_data = lgb.Dataset(test_df[features], label=test_df['score'], group=test_df['srch_id'].value_counts().sort_index())\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [3, 5],\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# 预测测试集\n",
    "test_pred = bst.predict(test_df[features])\n",
    "\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('/Users/eva/Documents/Study/Y1S2/DMT/assignment2/feature_engineered_test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "for chunk in reader:\n",
    "    # 可以在这里添加数据预处理步骤，比如填充缺失值等\n",
    "    chunk_pred = bst.predict(chunk[features])  # 应用模型进行预测\n",
    "    chunk['predictions'] = chunk_pred  # 将预测结果添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n",
    "\n",
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=all_featured_cleaned(train+set_all_grouped_by).csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
