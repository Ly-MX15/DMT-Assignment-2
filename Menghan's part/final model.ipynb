{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = 'D:/Table/P5/DM-AS2/Data_featured_menghan/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(10):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出df的前100行生成一个CSV文件\n",
    "df.head(500).to_csv('df_top_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 假设df已经加载并包含数据\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据train_ids划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = df[df['srch_id'].isin(train_ids)]\n",
    "val_df = df[df['srch_id'].isin(val_ids)]\n",
    "test_df = df[df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 提取特征和标签\n",
    "X_train = train_df[feature_columns].astype(np.float32)\n",
    "y_train = train_df['score'].astype(np.float32).values\n",
    "X_val = val_df[feature_columns].astype(np.float32)\n",
    "y_val = val_df['score'].astype(np.float32).values\n",
    "X_test = test_df[feature_columns].astype(np.float32)\n",
    "y_test = test_df['score'].astype(np.float32).values\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 转换为TensorFlow Dataset格式\n",
    "def make_dataset(X, y, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(y)).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),  # 确定学习NDCG@5\n",
    "    temperature=0.1,\n",
    "    ragged=False\n",
    ")\n",
    "\n",
    "# 构建DeepRank模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "input_shape = X_train.shape[1]\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "train_dataset = make_dataset(X_train, y_train, batch_size=32)\n",
    "val_dataset = make_dataset(X_val, y_val, batch_size=32)\n",
    "\n",
    "# 查看训练数据集中的一个批次\n",
    "for X, y in train_dataset.take(1):\n",
    "    X_batch = X.numpy()\n",
    "    y_batch = y.numpy()\n",
    "    X_shape = X.shape\n",
    "    y_shape = y.shape\n",
    "\n",
    "print(\"Feature batch (X):\", X_shape)\n",
    "print(\"Feature batch data (X):\", X_batch)\n",
    "print(\"Target batch (y):\", y_shape)\n",
    "print(\"Target batch data (y):\", y_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "# 对测试集进行预测和评估\n",
    "test_pred = model.predict(X_test).flatten()\n",
    "test_df['predictions'] = test_pred\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score@5: {average_ndcg}\")\n",
    "\n",
    "# 打印模型总结\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 假设 df 已经加载并包含数据\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = df[df['srch_id'].isin(train_ids)]\n",
    "val_df = df[df['srch_id'].isin(val_ids)]\n",
    "test_df = df[df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "val_df[feature_columns] = scaler.transform(val_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "def generator(dataframe):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        y = group['score'].values.astype(np.float32)\n",
    "        yield X, y[:, np.newaxis]  # 确保 y 是二维的\n",
    "\n",
    "def make_dataset(dataframe, batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(dataframe),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32)  # 确保 y 的形状是 (None, 1)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# 创建训练数据集\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size)\n",
    "\n",
    "# 查看训练数据集中的一个批次\n",
    "for X, y in train_dataset.take(1):\n",
    "    X_batch = X.numpy()\n",
    "    y_batch = y.numpy()\n",
    "    X_shape = X.shape\n",
    "    y_shape = y.shape\n",
    "\n",
    "print(\"Feature batch (X):\", X_shape)\n",
    "print(\"Feature batch data (X):\", X_batch)\n",
    "print(\"Target batch (y):\", y_shape)\n",
    "print(\"Target batch data (y):\", y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),  # 确定学习NDCG@5\n",
    "    temperature=0.1,\n",
    "    ragged=False\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "input_shape = len(feature_columns)\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "\n",
    "# 对测试集进行预测和评估\n",
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate(test_pred, axis=0)\n",
    "\n",
    "# 将预测结果与实际值进行对比，计算 NDCG@5\n",
    "test_scores = []\n",
    "test_groups = []\n",
    "\n",
    "for _, y in test_dataset:\n",
    "    test_scores.extend(y.numpy().flatten())  # 将y展平为一维数组\n",
    "    test_groups.append(len(y))\n",
    "\n",
    "ndcg_scores = []\n",
    "\n",
    "start = 0\n",
    "for group_size in test_groups:\n",
    "    end = start + group_size\n",
    "    true_relevance = test_scores[start:end]\n",
    "    pred_relevance = test_pred[start:end]\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [pred_relevance], k=5))\n",
    "    start = end\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score@5: {average_ndcg}\")\n",
    "\n",
    "# 打印模型总结\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = df[df['srch_id'].isin(train_ids)]\n",
    "val_df = df[df['srch_id'].isin(val_ids)]\n",
    "test_df = df[df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_columns] = scaler.fit_transform(train_df[feature_columns])\n",
    "val_df[feature_columns] = scaler.transform(val_df[feature_columns])\n",
    "test_df[feature_columns] = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "def generator(dataframe):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        y = group['score'].values.astype(np.float32)\n",
    "        yield tf.constant(X), tf.constant(y[:, np.newaxis])\n",
    "\n",
    "def make_dataset(dataframe, batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(dataframe),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset.padded_batch(batch_size, padded_shapes=([None, len(feature_columns)], [None, 1]))\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),  # 确定学习NDCG@5\n",
    "    temperature=0.1,\n",
    "    ragged=False  # 不使用 ragged=True\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=(None, len(feature_columns)), dtype=tf.float32)\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    output_layer = Reshape((-1, 1, 1))(output_layer)  # 调整输出形状为 [batch_size, list_size, 1, 1]\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "input_shape = (None, len(feature_columns))\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "# 对测试集进行预测和评估\n",
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate([pred.flatten() for pred in test_pred], axis=0)\n",
    "\n",
    "# 将预测结果与实际值进行对比，计算 NDCG@5\n",
    "test_scores = []\n",
    "test_groups = []\n",
    "\n",
    "for _, y in test_dataset:\n",
    "    test_scores.extend(y.numpy().flatten())  # 将y展平为一维数组\n",
    "    test_groups.append(len(y))\n",
    "\n",
    "ndcg_scores = []\n",
    "\n",
    "start = 0\n",
    "for group_size in test_groups:\n",
    "    end = start + group_size\n",
    "    true_relevance = test_scores[start:end]\n",
    "    pred_relevance = test_pred[start:end]\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [pred_relevance], k=5))\n",
    "    start = end\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score@5: {average_ndcg}\")\n",
    "\n",
    "# 打印模型总结\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def expand_to_fixed_length(group, length=50):\n",
    "    if len(group) < length:\n",
    "        # 创建一个填充 DataFrame，除了 srch_id 之外其他值均为0\n",
    "        padding = pd.DataFrame(0, index=np.arange(length - len(group)), columns=group.columns)\n",
    "        padding['srch_id'] = group['srch_id'].iloc[0]\n",
    "        group = pd.concat([group, padding], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "# 读取和处理数据\n",
    "base_path = 'D:/Table/P5/DM-AS2/Data_featured_menghan/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "df_list = []\n",
    "for i in range(1):  # 修改为实际的文件数量\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    df_list.append(df_chunk)\n",
    "\n",
    "# 合并所有数据\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# 扩展每个 srch_id 的数据到50条\n",
    "expanded_df = df.groupby('srch_id').apply(expand_to_fixed_length, length=50).reset_index(drop=True)\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in expanded_df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "expanded_df[feature_columns] = scaler.fit_transform(expanded_df[feature_columns])\n",
    "\n",
    "# 填充 NaN 值为 0\n",
    "expanded_df[feature_columns] = expanded_df[feature_columns].fillna(0)\n",
    "\n",
    "# 检查填充后是否还有 NaN 值\n",
    "nan_count_after_filling = expanded_df[feature_columns].isna().sum().sum()\n",
    "print(f'Number of NaN values after filling: {nan_count_after_filling}')\n",
    "\n",
    "# 确认处理后的数据长度一致\n",
    "print(expanded_df.groupby('srch_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 df 是原始数据 DataFrame\n",
    "# 统计每个 srch_id 的数据长度\n",
    "srch_id_length_distribution = df.groupby('srch_id').size()\n",
    "\n",
    "# 统计不同长度的分布情况\n",
    "length_distribution = srch_id_length_distribution.value_counts().sort_index()\n",
    "\n",
    "# 打印结果\n",
    "print(length_distribution)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 统计每个 srch_id 的数据条数\n",
    "srch_id_counts = df['srch_id'].value_counts()\n",
    "\n",
    "# 过滤掉数据量小于15的 srch_id\n",
    "valid_srch_ids = srch_id_counts[srch_id_counts >= 15].index\n",
    "filtered_df = df[df['srch_id'].isin(valid_srch_ids)]\n",
    "\n",
    "# 对于每个 srch_id，按照 score 值排序，并保留前15条记录\n",
    "processed_df = filtered_df.groupby('srch_id').apply(lambda x: x.nlargest(15, 'score')).reset_index(drop=True)\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in processed_df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "processed_df[feature_columns] = scaler.fit_transform(processed_df[feature_columns])\n",
    "\n",
    "# 确认处理后的数据长度一致\n",
    "print(processed_df.groupby('srch_id').size().value_counts())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生成和批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataframe):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        y = group['score'].values.astype(np.float32)\n",
    "        yield X, y\n",
    "\n",
    "def make_dataset(dataframe, batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(dataframe),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(50, len(feature_columns)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(50,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = expanded_df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = expanded_df[expanded_df['srch_id'].isin(train_ids)]\n",
    "val_df = expanded_df[expanded_df['srch_id'].isin(val_ids)]\n",
    "test_df = expanded_df[expanded_df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size)\n",
    "\n",
    "# 查看训练数据集中的一个批次\n",
    "for X, y in train_dataset.take(1):\n",
    "    X_shape = X.shape\n",
    "    y_shape = y.shape\n",
    "    X_batch = X.numpy()\n",
    "    y_batch = y.numpy()\n",
    "\n",
    "print(\"Feature batch (X) shape:\", X_shape)\n",
    "print(\"Target batch (y) shape:\", y_shape)\n",
    "print(\"Feature batch (X) data:\", X_batch)\n",
    "print(\"Target batch (y) data:\", y_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "expanded_df[feature_columns] = scaler.fit_transform(expanded_df[feature_columns])\n",
    "\n",
    "# 检查标准化后是否有 NaN 值\n",
    "print(expanded_df[feature_columns].isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),  # 确定学习NDCG@5\n",
    "    temperature=0.1,\n",
    "    ragged=False  # 不使用 ragged=True\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    output_layer = tf.squeeze(output_layer, axis=-1)  # 调整输出形状为 [batch_size, list_size]\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "input_shape = (50, len(feature_columns))\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 对测试集进行预测和评估\n",
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate(test_pred, axis=0)\n",
    "\n",
    "# 将预测结果与实际值进行对比，计算 NDCG@5\n",
    "test_scores = []\n",
    "test_groups = []\n",
    "\n",
    "# 提取实际评分和 srch_id\n",
    "for X, y in test_dataset:\n",
    "    test_scores.extend(y.numpy().flatten())  # 将y展平为一维数组\n",
    "    test_groups.extend([srch_id for srch_id in X.numpy()[:, 0, 0]])  # 假设 srch_id 是特征中的第一列\n",
    "\n",
    "# 确保 test_pred 和 test_scores 的长度一致\n",
    "assert len(test_pred) == len(test_scores)\n",
    "\n",
    "# 计算每个 srch_id 的 NDCG@5 分数\n",
    "ndcg_scores = []\n",
    "\n",
    "unique_ids = np.unique(test_groups)\n",
    "ly = 1\n",
    "for srch_id in unique_ids:\n",
    "    ly += 1\n",
    "    indices = [i for i, x in enumerate(test_groups) if x == srch_id]\n",
    "    true_relevance = [test_scores[i] for i in indices]\n",
    "    pred_relevance = [test_pred[i] for i in indices]\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [pred_relevance], k=5))\n",
    "print(ly)\n",
    "# 计算平均 NDCG@5 分数\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score@5: {average_ndcg}\")\n",
    "\n",
    "# 打印模型总结\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ndcg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_pred)\n",
    "print(len(test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = 'D:/Table/P5/DM-AS2/Data_featured_menghan/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(2):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个 srch_id 的数据条数\n",
    "srch_id_counts = df['srch_id'].value_counts()\n",
    "\n",
    "# 过滤掉数据量小于15的 srch_id\n",
    "valid_srch_ids = srch_id_counts[srch_id_counts >= 15].index\n",
    "filtered_df = df[df['srch_id'].isin(valid_srch_ids)]\n",
    "\n",
    "# 对于每个 srch_id，按照 score 值排序，并保留前15条记录\n",
    "processed_df = filtered_df.groupby('srch_id').apply(lambda x: x.nlargest(15, 'score')).reset_index(drop=True)\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in processed_df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "processed_df[feature_columns] = scaler.fit_transform(processed_df[feature_columns])\n",
    "\n",
    "# 确认处理后的数据长度一致\n",
    "print(processed_df.groupby('srch_id').size().value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataframe, is_train=True):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        if is_train:\n",
    "            y = group['score'].values.astype(np.float32)\n",
    "            yield X, y\n",
    "        else:\n",
    "            srch_id = group['srch_id'].values[0]\n",
    "            yield X, srch_id\n",
    "\n",
    "def make_dataset(dataframe, batch_size, is_train=True):\n",
    "    if is_train:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generator(dataframe, is_train),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generator(dataframe, is_train),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "            )\n",
    "        )\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = processed_df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = processed_df[processed_df['srch_id'].isin(train_ids)]\n",
    "val_df = processed_df[processed_df['srch_id'].isin(val_ids)]\n",
    "test_df = processed_df[processed_df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "batch_size = 1\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),\n",
    "    temperature=0.1,\n",
    "    ragged=False\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    output_layer = tf.squeeze(output_layer, axis=-1)  # 调整输出形状为 [batch_size, list_size]\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "input_shape = (15, len(feature_columns))\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate(test_pred, axis=0)\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "ly = 1\n",
    "for name, group in grouped:\n",
    "    ly += 1\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "print(ly)\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result output 等长数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the model and all setup are correctly initialized and available as earlier described\n",
    "\n",
    "# Reading the chunked data for prediction\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('D:/Table/P5/DM-AS2/Data_featured_menghan/best_feature_engineered_test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "predictions = []\n",
    "\n",
    "for chunk in reader:\n",
    "    # Data preprocessing (same as training data)\n",
    "    chunk[feature_columns] = scaler.transform(chunk[feature_columns])\n",
    "\n",
    "    # Dividing each srch_id data into groups of 15 records\n",
    "    grouped = chunk.groupby('srch_id')\n",
    "    temp_preds = []\n",
    "    for name, group in grouped:\n",
    "        n = len(group)\n",
    "        num_full_groups = n // 15\n",
    "        remainder = n % 15\n",
    "\n",
    "        # Predicting in full groups\n",
    "        for i in range(num_full_groups):\n",
    "            sub_group = group.iloc[i*15:(i+1)*15]\n",
    "            sub_group_dataset = make_dataset(sub_group, batch_size=1, is_train=False)\n",
    "            sub_group_pred = model.predict(sub_group_dataset)\n",
    "            temp_preds.extend(zip(sub_group['srch_id'], sub_group['prop_id'], np.concatenate(sub_group_pred, axis=0)))\n",
    "\n",
    "        # Handling the remainder data less than 15\n",
    "        if remainder > 0:\n",
    "            sub_group = group.iloc[num_full_groups*15:]\n",
    "            padding = 15 - remainder\n",
    "            # Filling the remainder with zeros or the mean of the dataset, or you might replicate the last few rows to make up numbers\n",
    "            padding_data = np.tile(sub_group[feature_columns].iloc[-1].values, (padding, 1))\n",
    "            sub_group_padded = pd.DataFrame(np.vstack([sub_group[feature_columns].values, padding_data]), columns=feature_columns)\n",
    "            sub_group_dataset = make_dataset(sub_group_padded, batch_size=1, is_train=False)\n",
    "            sub_group_pred = model.predict(sub_group_dataset)[:remainder]\n",
    "            temp_preds.extend(zip(sub_group['srch_id'], sub_group['prop_id'], np.concatenate(sub_group_pred, axis=0)))\n",
    "\n",
    "    # Collecting the predictions to a DataFrame\n",
    "    temp_df = pd.DataFrame(temp_preds, columns=['srch_id', 'prop_id', 'predictions'])\n",
    "    predictions.append(temp_df)\n",
    "\n",
    "# Concatenating all batches of predictions into a final DataFrame\n",
    "final_predictions = pd.concat(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=yuan_NN_init.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6 不等长数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# 逐个读取feature_engineered_training_chunk{i}并上下拼接到一个df\n",
    "base_path = 'D:/Table/P5/DM-AS2/Data_featured_menghan/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "for i in range(1):\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    if i == 0:\n",
    "        df = df_chunk\n",
    "    else:\n",
    "        df = pd.concat([df, df_chunk], axis=0)\n",
    "df.head()\n",
    "\n",
    "# 特征列，排除不需要的列\n",
    "feature_columns = [col for col in df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "# 定义生成器和数据集生成函数\n",
    "def generator(dataframe, is_train=True):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        if is_train:\n",
    "            y = group['score'].values.astype(np.float32)\n",
    "            yield X, y\n",
    "        else:\n",
    "            srch_id = group['srch_id'].values[0]\n",
    "            prop_id = group['prop_id'].values[0]\n",
    "            yield X, (srch_id, prop_id)\n",
    "\n",
    "def make_dataset(dataframe, batch_size, is_train=True):\n",
    "    if is_train:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generator(dataframe, is_train),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "        padded_shapes = ([None, len(feature_columns)], [None])\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            lambda: generator(dataframe, is_train),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(None, len(feature_columns)), dtype=tf.float32),\n",
    "                (tf.TensorSpec(shape=(), dtype=tf.int64), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
    "            )\n",
    "        )\n",
    "        padded_shapes = ([None, len(feature_columns)], ())\n",
    "\n",
    "    return dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取唯一的查询ID\n",
    "unique_ids = df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = df[df['srch_id'].isin(train_ids)]\n",
    "val_df = df[df['srch_id'].isin(val_ids)]\n",
    "test_df = df[df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size, is_train=False)\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),\n",
    "    temperature=0.1,\n",
    "    ragged=False\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    output_layer = tf.squeeze(output_layer, axis=-1)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建和训练模型\n",
    "input_shape = (None, len(feature_columns))\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate(test_pred, axis=0)\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/hfszkgw55n73h3v2jz7n8xth0000gn/T/ipykernel_10728/3533252856.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  expanded_df = df.groupby('srch_id').apply(expand_to_fixed_length, length=EXPAND_LENGTH).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values after filling: 0\n",
      "45    20058\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def assign_scores(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def expand_to_fixed_length(group, length=50):\n",
    "    if len(group) < length:\n",
    "        # 创建一个填充 DataFrame，除了 srch_id 之外其他值均为0\n",
    "        padding = pd.DataFrame(0, index=np.arange(length - len(group)), columns=group.columns)\n",
    "        padding['srch_id'] = group['srch_id'].iloc[0]\n",
    "        group = pd.concat([group, padding], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "# 读取和处理数据\n",
    "base_path = '/Users/eva/Documents/Study/Y1S2/DMT/assignment2/'\n",
    "file_pattern = 'best_feature_engineered_training_chunk_{}.csv'\n",
    "df_list = []\n",
    "for i in range(1):  # 修改为实际的文件数量\n",
    "    df_chunk = pd.read_csv(base_path + file_pattern.format(i))\n",
    "    df_chunk['score'] = df_chunk.apply(assign_scores, axis=1)\n",
    "    df_list.append(df_chunk)\n",
    "\n",
    "# 合并所有数据\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "EXPAND_LENGTH = 45\n",
    "# 扩展每个 srch_id 的数据到45条\n",
    "expanded_df = df.groupby('srch_id').apply(expand_to_fixed_length, length=EXPAND_LENGTH).reset_index(drop=True)\n",
    "\n",
    "columns = df.columns\n",
    "\n",
    "feature_columns = [\n",
    "    col for col in columns if col not in ['date_time', 'position', 'click_bool', 'booking_bool', 'score', 'srch_id']\n",
    "    and 'gross_bookings_usd' not in col and 'position' not in col and col != 'price_per_person_rank_percentile'\n",
    "]\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "expanded_df[feature_columns] = scaler.fit_transform(expanded_df[feature_columns])\n",
    "\n",
    "# 填充 NaN 值为 0\n",
    "expanded_df[feature_columns] = expanded_df[feature_columns].fillna(0)\n",
    "\n",
    "# 检查填充后是否还有 NaN 值\n",
    "nan_count_after_filling = expanded_df[feature_columns].isna().sum().sum()\n",
    "print(f'Number of NaN values after filling: {nan_count_after_filling}')\n",
    "\n",
    "# 确认处理后的数据长度一致\n",
    "print(expanded_df.groupby('srch_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生成和批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow_ranking as tfr\n",
    "def generator(dataframe):\n",
    "    grouped = dataframe.groupby('srch_id')\n",
    "    for name, group in grouped:\n",
    "        X = group[feature_columns].values.astype(np.float32)\n",
    "        y = group['score'].values.astype(np.float32)\n",
    "        yield X, y\n",
    "\n",
    "def make_dataset(dataframe, batch_size):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(dataframe),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(EXPAND_LENGTH, len(feature_columns)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(EXPAND_LENGTH,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# 获取唯一的查询ID\n",
    "unique_ids = expanded_df['srch_id'].unique()\n",
    "\n",
    "# 划分训练集和测试集的查询ID\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据 train_ids 划分训练集和验证集的查询ID\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# 根据查询ID过滤数据\n",
    "train_df = expanded_df[expanded_df['srch_id'].isin(train_ids)]\n",
    "val_df = expanded_df[expanded_df['srch_id'].isin(val_ids)]\n",
    "test_df = expanded_df[expanded_df['srch_id'].isin(test_ids)]\n",
    "\n",
    "# 创建训练、验证和测试数据集\n",
    "batch_size = 32\n",
    "train_dataset = make_dataset(train_df, batch_size).shuffle(buffer_size=len(train_df['srch_id'].unique()))\n",
    "val_dataset = make_dataset(val_df, batch_size).shuffle(buffer_size=len(val_df['srch_id'].unique()))\n",
    "test_dataset = make_dataset(test_df, batch_size)\n",
    "\n",
    "# 查看训练数据集中的一个批次\n",
    "for X, y in train_dataset.take(1):\n",
    "    X_shape = X.shape\n",
    "\n",
    "print(\"Feature batch (X) shape:\", X_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# 创建 ApproxNDCGLoss 对象\n",
    "loss = tfr.keras.losses.ApproxNDCGLoss(\n",
    "    reduction=tf.losses.Reduction.AUTO,\n",
    "    lambda_weight=tfr.keras.losses.DCGLambdaWeight(topn=5),  # 确定学习NDCG@5\n",
    "    temperature=0.1,\n",
    "    ragged=False  # 不使用 ragged=True\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(128, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.4)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    output_layer = tf.squeeze(output_layer, axis=-1)  # 调整输出形状为 [batch_size, list_size]\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "input_shape = (EXPAND_LENGTH, len(feature_columns))\n",
    "model = build_model(input_shape)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_dataset)\n",
    "test_pred = np.concatenate(test_pred, axis=0)\n",
    "# 评估模型，计算 NDCG 分数\n",
    "test_df['predictions'] = test_pred\n",
    "\n",
    "# 首先确保数据按照 srch_id 和 predictions 降序排序\n",
    "test_df.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 分组并计算每个搜索会话的 NDCG\n",
    "grouped = test_df.groupby('srch_id')\n",
    "ndcg_scores = []\n",
    "ly = 1\n",
    "for name, group in grouped:\n",
    "    ly += 1\n",
    "    group = group.sort_values('predictions', ascending=False)\n",
    "    true_relevance = group['score'].values\n",
    "    scores_pred = group['predictions'].values\n",
    "    # 计算当前搜索会话的 NDCG 分数，并追加到列表中\n",
    "    if len(np.unique(true_relevance)) > 1:  # 只计算有正样本的会话\n",
    "        ndcg_scores.append(ndcg_score([true_relevance], [scores_pred], k=5))\n",
    "print(ly)\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG Score: {average_ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result output 等长数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用迭代器逐块读取数据\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('D:/Table/P5/DM-AS2/Data_featured_menghan/best_feature_engineered_test_set_VU_DM.csv', chunksize=chunk_size)\n",
    "\n",
    "predictions = []  # 创建一个空列表以存储每个块的预测结果\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk['score'] = 0\n",
    "    # 计算评分并扩展数据到固定长度\n",
    "    chunk = chunk.groupby('srch_id').apply(expand_to_fixed_length, length=EXPAND_LENGTH).reset_index(drop=True)\n",
    "\n",
    "    # 特征列，排除不需要的列\n",
    "    feature_columns = [col for col in expanded_df.columns if col not in ['srch_id', 'date_time', 'position', 'click_bool', 'booking_bool', 'score', 'gross_bookings_usd']]\n",
    "\n",
    "    # 标准化特征数据\n",
    "    scaler = StandardScaler()\n",
    "    chunk[feature_columns] = scaler.fit_transform(chunk[feature_columns])\n",
    "\n",
    "    batch_size = 32\n",
    "    # 填充 NaN 值为 0\n",
    "    chunk[feature_columns] = chunk[feature_columns].fillna(0)\n",
    "    chunk_dataset = make_dataset(chunk, batch_size)\n",
    "    \n",
    "    # 应用模型进行预测\n",
    "    chunk_pred = model.predict(chunk_dataset)\n",
    "    chunk['predictions'] = chunk_pred.flatten()  # 将预测结果展平并添加到 DataFrame\n",
    "    predictions.append(chunk[['srch_id', 'prop_id', 'predictions']])  # 仅保留需要的列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有批次的预测结果\n",
    "final_predictions = pd.concat(predictions)\n",
    "\n",
    "# 确保按照预测分数排序，如果 Kaggle 要求\n",
    "final_predictions.sort_values(['srch_id', 'predictions'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 选择需要的列\n",
    "final_predictions = final_predictions[['srch_id', 'prop_id']]\n",
    "\n",
    "# 保存为 CSV 文件，确保不包含索引，包含列标题\n",
    "final_predictions.to_csv('train=yuan_NN_test_init.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
